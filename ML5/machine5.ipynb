{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "### This study compares logistic regression and decision tree classifiers using a specific dataset. Both models are evaluated based on accuracy and performance metrics. The objective is to determine which algorithm better suits the dataset, highlighting the strengths and weaknesses of each approach in a classification context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(r\"E:\\Datascience\\Data set\\drug200.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the first 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age Sex      BP Cholesterol  Na_to_K   Drug\n",
      "0    23   F    HIGH        HIGH   25.355  DrugY\n",
      "1    47   M     LOW        HIGH   13.093  drugC\n",
      "2    47   M     LOW        HIGH   10.114  drugC\n",
      "3    28   F  NORMAL        HIGH    7.798  drugX\n",
      "4    61   F     LOW        HIGH   18.043  DrugY\n",
      "5    22   F  NORMAL        HIGH    8.607  drugX\n",
      "6    49   F  NORMAL        HIGH   16.275  DrugY\n",
      "7    41   M     LOW        HIGH   11.037  drugC\n",
      "8    60   M  NORMAL        HIGH   15.171  DrugY\n",
      "9    43   M     LOW      NORMAL   19.368  DrugY\n",
      "10   47   F     LOW        HIGH   11.767  drugC\n",
      "11   34   F    HIGH      NORMAL   19.199  DrugY\n",
      "12   43   M     LOW        HIGH   15.376  DrugY\n",
      "13   74   F     LOW        HIGH   20.942  DrugY\n",
      "14   50   F  NORMAL        HIGH   12.703  drugX\n",
      "15   16   F    HIGH      NORMAL   15.516  DrugY\n",
      "16   69   M     LOW      NORMAL   11.455  drugX\n",
      "17   43   M    HIGH        HIGH   13.972  drugA\n",
      "18   23   M     LOW        HIGH    7.298  drugC\n",
      "19   32   F    HIGH      NORMAL   25.974  DrugY\n"
     ]
    }
   ],
   "source": [
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sex      BP Cholesterol   Drug\n",
      "0     F    HIGH        HIGH  DrugY\n",
      "1     M     LOW        HIGH  drugC\n",
      "2     M     LOW        HIGH  drugC\n",
      "3     F  NORMAL        HIGH  drugX\n",
      "4     F     LOW        HIGH  DrugY\n",
      "..   ..     ...         ...    ...\n",
      "195   F     LOW        HIGH  drugC\n",
      "196   M     LOW        HIGH  drugC\n",
      "197   M  NORMAL        HIGH  drugX\n",
      "198   M  NORMAL      NORMAL  drugX\n",
      "199   F     LOW      NORMAL  drugX\n",
      "\n",
      "[200 rows x 4 columns]\n",
      "(200, 6)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200 entries, 0 to 199\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Age          200 non-null    int64  \n",
      " 1   Sex          200 non-null    object \n",
      " 2   BP           200 non-null    object \n",
      " 3   Cholesterol  200 non-null    object \n",
      " 4   Na_to_K      200 non-null    float64\n",
      " 5   Drug         200 non-null    object \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 10.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r\"E:\\Datascience\\Data set\\drug200.csv\")\n",
    "df.columns = [col.replace(\" \", \"_\") for col in df.columns]\n",
    "df.dropna(inplace=True)\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "data_without_numerical = df.drop(columns=numerical_columns)\n",
    "print(data_without_numerical)\n",
    "categorical_column = 'categorical_column'\n",
    "if categorical_column in df.columns:\n",
    "    data_without_numerical[categorical_column].fillna('Unknown', inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)\n",
    "df.isna().sum()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.get_dummies(df,columns=[\"Sex\",\"BP\",\"Cholesterol\"],drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "y=df['Drug']\n",
    "x=df.drop('Drug',axis=1)\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.8,random_state=33)\n",
    "lr=LogisticRegression()\n",
    "model=lr.fit(x_train,y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DrugY'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[18,30,True,False,False,True]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Checking With Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree=DecisionTreeClassifier()\n",
    "model=tree.fit(x_train,y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here the logistic regression and decision tree algorithm had been performed amoung that decision tree algorithm has more accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "### Logistic regression and decision tree classifiers were applied to a given dataset to assess their predictive performance. Logistic regression, a linear model, is often used for binary classification due to its interpretability. In contrast, decision trees are non-linear models capable of capturing complex patterns. After training and testing both models, the decision tree achieved higher accuracy, indicating it better fits the underlying data structure. The comparison reveals that while logistic regression is simpler and computationally efficient, decision trees offer superior performance in datasets with non-linear relationships. The results underscore the importance of model selection based on data characteristics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "### The decision tree outperformed logistic regression in this analysis, achieving higher accuracy on the dataset. While logistic regression remains valuable for linear problems and interpretability, decision trees are more effective in handling complex patterns. This comparison emphasizes the need for data-driven model selection to achieve optimal predictive outcomes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
